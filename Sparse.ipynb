{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "条件数: 31942.033758957987\n",
      "最大奇异值: 709.0657622865946\n",
      "最小奇异值: 0.02219851646383476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 生成随机矩阵X_n\n",
    "n = 1000    # 样本量\n",
    "p = 1000    # 变量数\n",
    "\n",
    "# 创建协方差矩阵\n",
    "cov_matrix = np.full((p, p), 0.5)  # 创建一个所有元素都是 0.5 的矩阵\n",
    "np.fill_diagonal(cov_matrix, 2.0)  # 将对角线元素设置为 2.0\n",
    "\n",
    "mean_vector                = np.zeros(p)  # 均值向量\n",
    "X                          = np.random.multivariate_normal(mean_vector, cov_matrix, n)\n",
    "# 奇异值分解\n",
    "U, S, Vt = np.linalg.svd(X)\n",
    "\n",
    "# 计算条件数\n",
    "condition_number = S[0] / S[-1]\n",
    "print(\"条件数:\", condition_number)\n",
    "\n",
    "print(\"最大奇异值:\", S[0])\n",
    "print(\"最小奇异值:\", S[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14  20  79 152 223 251 284 423 544 593 622 637 687 740 746 819 826 866\n",
      " 937 981]\n"
     ]
    }
   ],
   "source": [
    "# 定义β向量\n",
    "beta = np.zeros(p)  # 初始化β向量为0\n",
    "\n",
    "# 根据条件设置β向量的值\n",
    "beta[0:5]   = 2.0   # βi = 2.0, i = 1, 2, 3, 4, 5\n",
    "beta[5:10]  = -2.0  # βi = -2.0, i = 6, 7, 8, 9, 10\n",
    "beta[10:15] = 1.0   # βi = 1.0, i = 11, 12, 13, 14, 15\n",
    "beta[15:20] = -1.0  # βi = -1.0, i = 16, 17, 18, 19, 20\n",
    "# beta[20:25] = 5.0   # βi = 5.0, i = 21, 22, 23, 24, 25\n",
    "\n",
    "# Generate 20 random indices\n",
    "indices = np.random.randint(low=0, high=p, size=20)\n",
    "\n",
    "# Get the indices of non-zero elements in beta\n",
    "nonzero_indices = np.nonzero(beta)[0]\n",
    "\n",
    "# print(nonzero_indices)\n",
    "\n",
    "# 初始化random_beta为零向量\n",
    "random_beta = np.zeros(p)\n",
    "\n",
    "# Assign the non-zero elements randomly\n",
    "# 此处假设的意图是将beta中的非零元素随机分配到random_beta中\n",
    "# 但这样的操作逻辑上存在问题，因为indices的长度和nonzero_indices可能不同\n",
    "# 为了简化，这里我们只是简单地复制beta到random_beta\n",
    "random_beta[indices] = beta[nonzero_indices]\n",
    "\n",
    "# Print the randomly assigned beta\n",
    "# print(random_beta)\n",
    "beta = random_beta\n",
    "nonzero_indices = np.nonzero(beta)[0]\n",
    "print(nonzero_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.04652964580067\n"
     ]
    }
   ],
   "source": [
    "# 生成均值为0方差为4的n维向量en\n",
    "mean    = 0  # 均值\n",
    "std_dev = 2  # 标准差，方差为标准差的平方，即4\n",
    "\n",
    "# 生成向量\n",
    "e = np.random.normal(mean, std_dev, n)\n",
    "norm_e = np.linalg.norm(e)\n",
    "Y = np.dot(X, beta) + e\n",
    "print(norm_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau   = 1\n",
    "delta = tau*norm_e\n",
    "s1, s2 = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.30149225611223\n",
      "1.2081041155449896e-13\n"
     ]
    }
   ],
   "source": [
    "# 最小二乘法\n",
    "def least_square(X, Y):\n",
    "    beta = np.linalg.lstsq(X, Y, rcond=None)[0]\n",
    "    return beta\n",
    "beta_LS = least_square(X, Y)\n",
    "norm_LS = np.linalg.norm(beta_LS - beta)\n",
    "print(norm_LS)\n",
    "sigma_LS = np.linalg.norm(Y - np.dot(X, beta_LS))\n",
    "sigma_LS = sigma_LS / np.sqrt(s2)\n",
    "print(sigma_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 奇异值分解\n",
    "U, S, Vt = np.linalg.svd(X)\n",
    "\n",
    "# 定义一个函数来计算 norm_cut\n",
    "def compute_norm_cut(k):\n",
    "    # 截断奇异值\n",
    "    S_truncated = np.zeros_like(S)\n",
    "    S_truncated[:k] = S[:k]\n",
    "\n",
    "    # 构造截断后的矩阵\n",
    "    Sigma_truncated = np.zeros((U.shape[0], Vt.shape[0]))\n",
    "    np.fill_diagonal(Sigma_truncated, S_truncated)\n",
    "    X_truncated = U @ Sigma_truncated @ Vt\n",
    "\n",
    "    # 计算 S_cut 和 S_inv\n",
    "    S_cut = Sigma_truncated.T @ Sigma_truncated\n",
    "    S_inv = np.zeros_like(S_cut)\n",
    "    S_inv[S_cut != 0] = 1 / S_cut[S_cut != 0]\n",
    "\n",
    "    # 计算 beta_cut\n",
    "    beta_cut = Vt.T @ S_inv @ Vt @ X.T @ Y\n",
    "\n",
    "    # 计算 norm_cut\n",
    "    norm_cut = np.linalg.norm(beta_cut - beta)\n",
    "    \n",
    "    return norm_cut, k\n",
    "\n",
    "# 使用Parallel和delayed并行处理\n",
    "results = Parallel(n_jobs=-1)(delayed(compute_norm_cut)(k) for k in range(1000, 0, -1))\n",
    "\n",
    "# 找到最小的 norm_cut 及其对应的 k 值\n",
    "min_norm_cut, best_k = min(results, key=lambda x: x[0])\n",
    "\n",
    "# 输出最小的 norm_cut 及其对应的 k 值\n",
    "print(f\"最小的 norm_cut: {min_norm_cut}, 对应的 k: {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral cut-off regression\n",
    "\n",
    "# 奇异值分解\n",
    "U, S, Vt = np.linalg.svd(X)\n",
    "\n",
    "# 选择截断点\n",
    "k = best_k  # 截断点，可以根据需要调整\n",
    "\n",
    "# 截断奇异值\n",
    "S_truncated = np.zeros_like(S)\n",
    "S_truncated[:k] = S[:k]\n",
    "\n",
    "# 构造截断后的矩阵\n",
    "Sigma = np.zeros((U.shape[0], Vt.shape[0]))\n",
    "Sigma_truncated = np.zeros((U.shape[0], Vt.shape[0]))\n",
    "np.fill_diagonal(Sigma, S)\n",
    "np.fill_diagonal(Sigma_truncated, S_truncated)\n",
    "X_truncated = U @ Sigma_truncated @ Vt\n",
    "S_cut = Sigma_truncated.T@Sigma_truncated\n",
    "S_inv = np.zeros_like(S_cut)\n",
    "S_inv[S_cut != 0] = 1 / S_cut[S_cut != 0]\n",
    "beta_cut = Vt.T @ S_inv @ Vt @ X.T @ Y\n",
    "norm_cut = np.linalg.norm(beta_cut - beta)\n",
    "I = np.eye(s2)\n",
    "beta_cut_debias =(2*I -Sigma.T@ Sigma @ S_inv) @ beta_cut\n",
    "norm_cut_debias = np.linalg.norm(beta_cut_debias - beta)\n",
    "print(norm_cut, norm_cut_debias)\n",
    "sigma_cut = np.linalg.norm(Y - X @ beta_cut)\n",
    "sigma_cut = sigma_cut / np.sqrt(s2)\n",
    "print(sigma_cut)\n",
    "sigma_cut_debias = np.linalg.norm(Y - X @ beta_cut_debias)\n",
    "sigma_cut_debias = sigma_cut_debias / np.sqrt(s2)\n",
    "print(sigma_cut_debias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of C that minimizes the mean squared error is: 95000\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "C = 200000\n",
    "m = 100\n",
    "def compute_mse(c, X, Y):\n",
    "    alpha = c / s1\n",
    "    I = np.eye(s2)\n",
    "    beta_r = np.linalg.inv(X.T @ X + alpha * I) @ X.T @ Y\n",
    "    mse = np.linalg.norm(beta_r - beta)\n",
    "    return mse\n",
    "\n",
    "# 假设 X, Y, C 已经定义\n",
    "mse_list = Parallel(n_jobs=-1)(delayed(compute_mse)(c, X, Y) for c in range(0, C + 1, m))\n",
    "\n",
    "best_c = np.argmin(mse_list)*m\n",
    "print(\"The value of C that minimizes the mean squared error is:\", best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.211954981423207 3.4416709271979413\n",
      "1.0130884813345848\n",
      "0.6417268117639816\n"
     ]
    }
   ],
   "source": [
    "# 岭回归\n",
    "def ridge(X, Y, C):\n",
    "    alpha  = C/s1\n",
    "    I      = np.eye(s2)\n",
    "    gen    = np.linalg.inv(X.T @ X + alpha*I) @ X.T\n",
    "    beta   = gen @ Y\n",
    "    beta_debias = (I + alpha*np.linalg.inv(X.T @ X + alpha*I))@beta\n",
    "    return beta, beta_debias\n",
    "\n",
    "beta_Ridge, beta_Ridge_debias, gen_Ridge = ridge(X, Y, best_c)\n",
    "norm_Ridge = np.linalg.norm(beta_Ridge - beta)\n",
    "norm_Ridge_debias = np.linalg.norm(beta_Ridge_debias - beta)\n",
    "print(norm_Ridge, norm_Ridge_debias)\n",
    "sigma_Ridge = np.linalg.norm(Y - np.dot(X, beta_Ridge))\n",
    "sigma_Ridge = sigma_Ridge / np.sqrt(s2)\n",
    "print(sigma_Ridge)\n",
    "sigma_Ridge_debias = np.linalg.norm(Y - np.dot(X, beta_Ridge_debias))\n",
    "sigma_Ridge_debias = sigma_Ridge_debias / np.sqrt(s2)\n",
    "print(sigma_Ridge_debias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "C = 1000\n",
    "m = 1\n",
    "def compute_mse(c, X, Y):\n",
    "    alpha = c / s1\n",
    "    lasso =  Lasso(alpha=alpha)\n",
    "    lasso.fit(X, Y)\n",
    "    beta_lasso = lasso.coef_\n",
    "    mse = np.linalg.norm(beta_lasso - beta)\n",
    "    return mse\n",
    "\n",
    "# 假设 X, Y, C 已经定义\n",
    "mse_list = Parallel(n_jobs=-1)(delayed(compute_mse)(c, X, Y) for c in range(0, C + 1, m))\n",
    "\n",
    "best_c = np.argmin(mse_list)*m\n",
    "print(\"The value of C is:\", best_c)\n",
    "lasso =  Lasso(alpha=best_c)\n",
    "lasso.fit(X, Y)\n",
    "beta_lasso = lasso.coef_\n",
    "norm_Lasso = np.linalg.norm(beta_lasso - beta)\n",
    "print(norm_Lasso)\n",
    "sigma_Lasso = np.linalg.norm(Y - np.dot(X, beta_lasso))\n",
    "sigma_Lasso = sigma_Lasso / np.sqrt(s2)\n",
    "print(sigma_Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.643950876190539 1217 4.101288265260092\n",
      "3.0207964006327366\n",
      "2.0200968999970437\n"
     ]
    }
   ],
   "source": [
    "# Landweber回归\n",
    "t        = 5e-7    # 步长\n",
    "max_iter = 10000   # 最大迭代次数\n",
    "\n",
    "def landweber(X, Y, t, max_iter):\n",
    "    s = X.shape\n",
    "    m = s[1]\n",
    "    beta_cur = np.zeros(m)\n",
    "    for k in range(1, max_iter):\n",
    "        beta_prev = beta_cur\n",
    "        beta_cur = beta_prev + t * np.dot(X.T, (Y - np.dot(X, beta_prev)))\n",
    "        if np.linalg.norm(Y - np.dot(X, beta_cur)) <= 1.5*delta:\n",
    "            break\n",
    "    return beta_cur, k\n",
    "def landweber_debias(X, Y, t, max_iter):\n",
    "    s = X.shape\n",
    "    m = s[1]\n",
    "    beta_cur = np.zeros(m)\n",
    "    for k in range(1, max_iter):\n",
    "        beta_prev = beta_cur\n",
    "        beta_cur = beta_prev + t * np.dot(X.T, (Y - np.dot(X, beta_prev)))\n",
    "        if k == max_iter:\n",
    "            break\n",
    "    return beta_cur\n",
    "beta_Landweber, k_Landweber = landweber(X, Y, t, max_iter)\n",
    "beta_Landweber_debias = landweber_debias(X, Y, t, 2*k_Landweber)\n",
    "norm_Landweber    = np.linalg.norm(beta_Landweber - beta)\n",
    "norm_Landweber_debias  = np.linalg.norm(beta_Landweber_debias - beta)  \n",
    "\n",
    "print(norm_Landweber, k_Landweber, norm_Landweber_debias)\n",
    "sigma_Landeweber = np.linalg.norm(Y - np.dot(X, beta_Landweber))\n",
    "sigma_Landeweber = sigma_Landeweber / np.sqrt(s2)\n",
    "print(sigma_Landeweber)\n",
    "sigma_Landeweber_debias = np.linalg.norm(Y - np.dot(X, beta_Landweber_debias))\n",
    "sigma_Landeweber_debias = sigma_Landeweber_debias / np.sqrt(s2)\n",
    "print(sigma_Landeweber_debias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showalter回归\n",
    "t        = 5e-7    # 步长\n",
    "max_iter = 10000   # 最大迭代次数\n",
    "\n",
    "def showalter(X, Y, t, max_iter):\n",
    "    s = X.shape\n",
    "    m = s[1]\n",
    "    beta_cur = np.zeros(m)\n",
    "    Z1 = np.dot(X.T, Y)\n",
    "    Z2 = np.dot(X.T, X)\n",
    "    for k in range(1, max_iter):\n",
    "        beta_prev = beta_cur\n",
    "        K1 = Z1 - np.dot(Z2, beta_prev)\n",
    "        K2 = Z1 - np.dot(Z2, (beta_prev + 0.5 * t * K1))\n",
    "        K3 = Z1 - np.dot(Z2, (beta_prev + 0.5 * t * K2))\n",
    "        K4 = Z1 - np.dot(Z2, (beta_prev + t * K3))\n",
    "        beta_cur = beta_prev + t / 6 * (K1 + 2 * K2 + 2 * K3 + K4)\n",
    "        # print(np.linalg.norm(Y - np.dot(X, beta_cur)))\n",
    "        # if np.linalg.norm(beta_cur - beta_prev) <= 1e-4:\n",
    "        if np.linalg.norm(Y - np.dot(X, beta_cur)) <= delta:\n",
    "            break\n",
    "    return beta_cur, k\n",
    "def showalter_debias(X, Y, t, max_iter):\n",
    "    s = X.shape\n",
    "    m = s[1]\n",
    "    beta_cur = np.zeros(m)\n",
    "    Z1 = np.dot(X.T, Y)\n",
    "    Z2 = np.dot(X.T, X)\n",
    "    for k in range(1, max_iter):\n",
    "        beta_prev = beta_cur\n",
    "        K1 = Z1 - np.dot(Z2, beta_prev)\n",
    "        K2 = Z1 - np.dot(Z2, (beta_prev + 0.5 * t * K1))\n",
    "        K3 = Z1 - np.dot(Z2, (beta_prev + 0.5 * t * K2))\n",
    "        K4 = Z1 - np.dot(Z2, (beta_prev + t * K3))\n",
    "        beta_cur = beta_prev + t / 6 * (K1 + 2 * K2 + 2 * K3 + K4)\n",
    "        if k == max_iter:\n",
    "            break\n",
    "    return beta_cur\n",
    "beta_showalter, k_showalter = showalter(X, Y, t, max_iter)\n",
    "beta_showalter_debias = showalter_debias(X, X@beta_showalter, t, k_showalter)\n",
    "beta_showalter_debias = 2*beta_showalter-beta_showalter_debias\n",
    "norm_showalter = np.linalg.norm(beta_showalter - beta)\n",
    "norm_showalter_debias = np.linalg.norm(beta_showalter_debias - beta)\n",
    "print(norm_showalter, k_showalter, norm_showalter_debias)\n",
    "sigma_showalter = np.linalg.norm(Y - np.dot(X, beta_showalter))\n",
    "sigma_showalter = sigma_showalter / np.sqrt(s2)\n",
    "print(sigma_showalter)\n",
    "sigma_showalter_debias = np.linalg.norm(Y - np.dot(X, beta_showalter_debias))\n",
    "sigma_showalter_debias = sigma_showalter_debias / np.sqrt(s2)\n",
    "print(sigma_showalter_debias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showalter回归\n",
    "t        = 5e-7    # 步长\n",
    "max_iter = 10000   # 最大迭代次数\n",
    "\n",
    "def showalter(X, Y, t, max_iter):\n",
    "    s = X.shape\n",
    "    m = s[1]\n",
    "    beta_cur = np.zeros(m)\n",
    "    Z1 = np.dot(X.T, Y)\n",
    "    Z2 = np.dot(X.T, X)\n",
    "    for k in range(1, max_iter):\n",
    "        beta_prev = beta_cur\n",
    "        K1 = Z1 - np.dot(Z2, beta_prev)\n",
    "        K2 = Z1 - np.dot(Z2, (beta_prev + 0.5 * t * K1))\n",
    "        K3 = Z1 - np.dot(Z2, (beta_prev + 0.5 * t * K2))\n",
    "        K4 = Z1 - np.dot(Z2, (beta_prev + t * K3))\n",
    "        beta_cur = beta_prev + t / 6 * (K1 + 2 * K2 + 2 * K3 + K4)\n",
    "        # print(np.linalg.norm(Y - np.dot(X, beta_cur)))\n",
    "        # if np.linalg.norm(beta_cur - beta_prev) <= 1e-4:\n",
    "        if np.linalg.norm(Y - np.dot(X, beta_cur)) <= delta:\n",
    "            break\n",
    "    return beta_cur, k\n",
    "def showalter_debias(X, Y, t, max_iter):\n",
    "    s = X.shape\n",
    "    m = s[1]\n",
    "    beta_cur = np.zeros(m)\n",
    "    Z1 = np.dot(X.T, Y)\n",
    "    Z2 = np.dot(X.T, X)\n",
    "    for k in range(1, max_iter):\n",
    "        beta_prev = beta_cur\n",
    "        K1 = Z1 - np.dot(Z2, beta_prev)\n",
    "        K2 = Z1 - np.dot(Z2, (beta_prev + 0.5 * t * K1))\n",
    "        K3 = Z1 - np.dot(Z2, (beta_prev + 0.5 * t * K2))\n",
    "        K4 = Z1 - np.dot(Z2, (beta_prev + t * K3))\n",
    "        beta_cur = beta_prev + t / 6 * (K1 + 2 * K2 + 2 * K3 + K4)\n",
    "        if k == max_iter:\n",
    "            break\n",
    "    return beta_cur\n",
    "beta_showalter, k_showalter = showalter(X, Y, t, max_iter)\n",
    "beta_showalter_debias = showalter_debias(X, X@beta_showalter, t, k_showalter)\n",
    "beta_showalter_debias = 2*beta_showalter-beta_showalter_debias\n",
    "norm_showalter = np.linalg.norm(beta_showalter - beta)\n",
    "norm_showalter_debias = np.linalg.norm(beta_showalter_debias - beta)\n",
    "print(norm_showalter, k_showalter, norm_showalter_debias)\n",
    "sigma_showalter = np.linalg.norm(Y - np.dot(X, beta_showalter))\n",
    "sigma_showalter = sigma_showalter / np.sqrt(s2)\n",
    "print(sigma_showalter)\n",
    "sigma_showalter_debias = np.linalg.norm(Y - np.dot(X, beta_showalter_debias))\n",
    "sigma_showalter_debias = sigma_showalter_debias / np.sqrt(s2)\n",
    "print(sigma_showalter_debias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceleration regression of order kappa\n",
    "t        = 5e-5   # 步长\n",
    "max_iter = 1000   # 最大迭代次数\n",
    "kappa    = 1.5       # 加速阶数\n",
    "\n",
    "def ARk(X, Y, t, k_max):\n",
    "    s = X.shape\n",
    "    m = s[1]\n",
    "    beta_prev = np.zeros(m)\n",
    "    I      = np.eye(m)\n",
    "    v_prev = np.zeros(m)\n",
    "    for k in range(1, k_max):\n",
    "        beta_cur = beta_prev + k*t*v_prev\n",
    "        D = np.linalg.inv((t*(k*t)**kappa)*X.T @ X + I + (((k*t)**(-kappa))-kappa)/k*I)\n",
    "        v_cur = D@v_prev+ t*D@np.dot(X.T, (Y - np.dot(X, beta_prev)))\n",
    "        beta_prev = beta_cur\n",
    "        v_prev = v_cur\n",
    "        print(np.linalg.norm(Y - np.dot(X, beta_cur)),k)\n",
    "        if np.linalg.norm(Y - np.dot(X, beta_prev)) <= delta:\n",
    "            break\n",
    "    return beta_cur, k\n",
    "\n",
    "def ARk_debias(X, Y, t, k_max):\n",
    "    s = X.shape\n",
    "    m = s[1]\n",
    "    beta_prev = np.zeros(m)\n",
    "    I      = np.eye(m)\n",
    "    v_prev = np.zeros(m)\n",
    "    for k in range(1, k_max):\n",
    "        beta_cur = beta_prev + k*t*v_prev\n",
    "        D = np.linalg.inv((t*(k*t)**kappa)*X.T @ X + I + (((k*t)**(-kappa))-kappa)/k*I)\n",
    "        v_cur = D@v_prev+t*D@np.dot(X.T, (Y - np.dot(X, beta_prev)))\n",
    "        beta_prev = beta_cur\n",
    "        v_prev = v_cur\n",
    "        if k == max_iter:\n",
    "            break\n",
    "    return beta_cur    \n",
    "\n",
    "beta_ARk, k_ARk = ARk(X, Y, t, max_iter)\n",
    "beta_ARk_debias = ARk_debias(X, X@beta_ARk, t, k_ARk)\n",
    "beta_ARk_debias = 2*beta_ARk - beta_ARk_debias\n",
    "norm_ARk = np.linalg.norm(beta_ARk - beta)\n",
    "norm_ARk_debias = np.linalg.norm(beta_ARk_debias - beta)\n",
    "print(norm_ARk, k_ARk, norm_ARk_debias)\n",
    "sigma_ARk = np.linalg.norm(Y - np.dot(X, beta_ARk))\n",
    "sigma_ARk = sigma_ARk / np.sqrt(s2)\n",
    "print(sigma_ARk)\n",
    "sigma_ARk_debias = np.linalg.norm(Y - np.dot(X, beta_ARk_debias))\n",
    "sigma_ARk_debias = sigma_ARk_debias / np.sqrt(s2)\n",
    "print(sigma_ARk_debias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二阶渐进回归\n",
    "t        = 5e-4    # 步长\n",
    "max_iter = 10000   # 最大迭代次数\n",
    "\n",
    "def second(X, Y, t, k_max):\n",
    "    r = X.shape\n",
    "    m = r[1]\n",
    "    s = 0.5\n",
    "    beta_cur = np.zeros(m)\n",
    "    v_cur    = np.zeros(m)\n",
    "    q_cur    = np.zeros(m)\n",
    "    for k in range(1, k_max):\n",
    "        p        =  2*k/(2*k+2*s+1)\n",
    "        q_half   =  p*q_cur + p*(t / 2) * np.dot(X.T, (Y - np.dot(X, beta_cur))) \n",
    "        beta_cur = beta_cur + t * q_half\n",
    "        a        = (2*k-2*s+1)*(k+1)/(2*k+2*s+3)/k\n",
    "        v_cur   =  beta_cur + 2*t*a*q_half\n",
    "        q_cur    =  (2*k+1-2*s)/(2*k+2)*q_half +(t / 2) * np.dot(X.T, (Y - np.dot(X, v_cur))) \n",
    "        # print(np.linalg.norm(Y - np.dot(X, beta_cur)))\n",
    "        if np.linalg.norm(Y - np.dot(X, beta_cur)) <= delta:\n",
    "            break\n",
    "    return beta_cur, k\n",
    "\n",
    "def second_debias(X, Y, t, k_max):\n",
    "    r = X.shape\n",
    "    m = r[1]\n",
    "    s = 0.5\n",
    "    beta_cur = np.zeros(m)\n",
    "    v_cur    = np.zeros(m)\n",
    "    q_cur    = np.zeros(m)\n",
    "\n",
    "    for k in range(1, k_max):\n",
    "        p        =  2*k/(2*k+2*s+1)\n",
    "        q_half   =  p*q_cur + p*(t / 2) * np.dot(X.T, (Y - np.dot(X, beta_cur))) \n",
    "        beta_cur = beta_cur + t * q_half\n",
    "        a        = (2*k-2*s+1)*(k+1)/(2*k+2*s+3)/k\n",
    "        v_cur   =  beta_cur + 2*t*a*q_half\n",
    "        q_cur    =  (2*k+1-2*s)/(2*k+2)*q_half +(t / 2) * np.dot(X.T, (Y - np.dot(X, v_cur))) \n",
    "        if k == k_max:\n",
    "            break\n",
    "    return beta_cur\n",
    "\n",
    "beta_second, k_second = second(X, Y, t, max_iter)\n",
    "beta_second_debias = second_debias(X, X@beta_second, t, k_second)\n",
    "beta_second_debias = 2*beta_second - beta_second_debias\n",
    "norm_second = np.linalg.norm(beta_second - beta)\n",
    "norm_second_debias = np.linalg.norm(beta_second_debias - beta)\n",
    "print(norm_second, k_second, norm_second_debias)\n",
    "sigma_second = np.linalg.norm(Y - np.dot(X, beta_second))\n",
    "sigma_second = sigma_second / np.sqrt(s2)\n",
    "print(sigma_second)\n",
    "sigma_second_debias = np.linalg.norm(Y - np.dot(X, beta_second_debias))\n",
    "sigma_second_debias = sigma_second_debias / np.sqrt(s2)\n",
    "print(sigma_second_debias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nesterov回归\n",
    "t        = 5e-7    # 步长\n",
    "max_iter = 10000   # 最大迭代次数\n",
    "omega    = 3       # omega  \n",
    "\n",
    "def Nesterov(X, Y, t, k_max):\n",
    "    s = X.shape\n",
    "    m = s[1]\n",
    "    beta_cur = np.zeros(m)\n",
    "    for k in range(1, k_max):\n",
    "        beta_prev = beta_cur\n",
    "        z_k = beta_cur + (k - 1) / (k + omega) * (beta_cur - beta_prev)\n",
    "        XTX_z_k = np.dot(X.T, (Y - np.dot(X, z_k)))\n",
    "        beta_cur = z_k + t * XTX_z_k\n",
    "        # print(np.linalg.norm(Y - np.dot(X, beta_cur)))\n",
    "        if np.linalg.norm(Y - np.dot(X, beta_cur)) <= delta:\n",
    "            break\n",
    "    return beta_cur, k+1\n",
    "\n",
    "def Nesterov_debias(X, Y, t, k_max):\n",
    "    s = X.shape\n",
    "    m = s[1]\n",
    "    beta_cur = np.zeros(m)\n",
    "    for k in range(1, k_max):\n",
    "        beta_prev = beta_cur\n",
    "        z_k = beta_cur + (k - 1) / (k + omega) * (beta_cur - beta_prev)\n",
    "        XTX_z_k = np.dot(X.T, (Y - np.dot(X, z_k)))\n",
    "        beta_cur = z_k + t * XTX_z_k\n",
    "        if k == k_max:\n",
    "            break\n",
    "    return beta_cur\n",
    "\n",
    "beta_Nesterov, k_Nesterov = Nesterov(X, Y, t, max_iter)\n",
    "beta_Nesterov_debias = Nesterov_debias(X, X@beta_Nesterov, t, k_Nesterov)\n",
    "beta_Nesterov_debias = 2*beta_Nesterov - beta_Nesterov_debias\n",
    "norm_Nesterov = np.linalg.norm(beta_Nesterov - beta)\n",
    "norm_Nesterov_debias = np.linalg.norm(beta_Nesterov_debias - beta)\n",
    "print(norm_Nesterov, k_Nesterov, norm_Nesterov_debias)\n",
    "sigma_Nesterov = np.linalg.norm(Y - np.dot(X, beta_Nesterov))\n",
    "sigma_Nesterov = sigma_Nesterov / np.sqrt(s2)\n",
    "print(sigma_Nesterov)\n",
    "sigma_Nesterov_debias = np.linalg.norm(Y - np.dot(X, beta_Nesterov_debias))\n",
    "sigma_Nesterov_debias = sigma_Nesterov_debias / np.sqrt(s2)\n",
    "print(sigma_Nesterov_debias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "\n",
    "# 分数阶回归\n",
    "t        = 5e-5     # 步长\n",
    "max_iter = 500      # 最大迭代次数\n",
    "theta    = 1.5       # theta\n",
    "A        = np.zeros((max_iter+1, max_iter+1))\n",
    "B        = np.zeros((max_iter, max_iter+1))\n",
    "g        = 1/gamma(theta)\n",
    "k1       = t**(theta)/(theta*(theta+1))\n",
    "k2       = t**(theta)/theta\n",
    "\n",
    "for k in range(max_iter):\n",
    "    for j in range(k+2):\n",
    "        if j == 0:\n",
    "            d = k**(theta+1)-(k-theta)*(k+1)**(theta)\n",
    "            A[j, k+1] = k1*d\n",
    "        elif j < k+1:\n",
    "            d = (k-j+2)**(theta+1)+(k-j)**(theta+1)-2*(k-j+1)**(theta+1)\n",
    "            A[j, k+1] = k1*d\n",
    "        elif j == k+1:\n",
    "            A[j, k+1] = k1 \n",
    "    for i in range(k+1):\n",
    "        if i < k+1:\n",
    "            d = (k-i+1)**theta-(k-i)**theta  \n",
    "            B[i, k+1] = k2*d\n",
    "\n",
    "\n",
    "def Frac(X, Y, k_max):\n",
    "    s  = X.shape\n",
    "    m  = s[1]\n",
    "    b  = np.zeros((m, k_max+1))  \n",
    "    bp = np.zeros((m, k_max+1))\n",
    "    for k in range(1, k_max):\n",
    "        bp[:, k+1] = g*sum([B[j, k+1]*np.dot(X.T, (Y - np.dot(X, b[:,j]))) for j in range(1,k+1)]) \n",
    "        b[:, k+1]  = g*(A[k+1, k+1]*np.dot(X.T, (Y - np.dot(X, bp[:,k+1])))+sum([A[j, k+1]*np.dot(X.T, (Y - np.dot(X, b[:,j]))) for j in range(1, k+1)]))\n",
    "        beta_cur   = b[:, k+1]\n",
    "        # print(np.linalg.norm(Y - np.dot(X, beta_cur)), k)\n",
    "        if np.linalg.norm(Y - np.dot(X, beta_cur)) <= delta:\n",
    "            break\n",
    "    return beta_cur, k\n",
    "def Frac_debias(X, Y, k_max):\n",
    "    s  = X.shape\n",
    "    m  = s[1]\n",
    "    b  = np.zeros((m, k_max+1))  \n",
    "    bp = np.zeros((m, k_max+1))\n",
    "    for k in range(1, k_max):\n",
    "        bp[:, k+1] = g*sum([B[j, k+1]*np.dot(X.T, (Y - np.dot(X, b[:,j]))) for j in range(1,k+1)]) \n",
    "        b[:, k+1]  = g*(A[k+1, k+1]*np.dot(X.T, (Y - np.dot(X, bp[:,k+1])))+sum([A[j, k+1]*np.dot(X.T, (Y - np.dot(X, b[:,j]))) for j in range(1, k+1)]))\n",
    "        beta_cur   = b[:, k+1]\n",
    "        if k == k_max:\n",
    "            break\n",
    "    return beta_cur\n",
    "\n",
    "beta_Frac, k_Frac = Frac(X, Y, max_iter)\n",
    "beta_Frac_debias = Frac_debias(X, X@beta_Frac, k_Frac)\n",
    "beta_Frac_debias = 2*beta_Frac - beta_Frac_debias\n",
    "norm_Frac = np.linalg.norm(beta_Frac - beta)\n",
    "norm_Frac_debias = np.linalg.norm(beta_Frac_debias - beta)\n",
    "print(norm_Frac, k_Frac, norm_Frac_debias)\n",
    "sigma_Frac = np.linalg.norm(Y - np.dot(X, beta_Frac))\n",
    "sigma_Frac = sigma_Frac / np.sqrt(s2)\n",
    "print(sigma_Frac)\n",
    "sigma_Frac_debias = np.linalg.norm(Y - np.dot(X, beta_Frac_debias))\n",
    "sigma_Frac_debias = sigma_Frac_debias / np.sqrt(s2)\n",
    "print(sigma_Frac_debias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
